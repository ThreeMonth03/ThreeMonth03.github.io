<!DOCTYPE html>

<html lang="zh-TW,en,default">

<head>
    
    <title>為什麼torch.nn.transformer中每個input的feature size需要是head數量的倍數 - 三個月的小屋</title>
    <meta charset="UTF-8">
    <meta name="keywords" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/vs2015.min.css"> <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <link rel="shortcut icon" href="../images/head/new.png" type="image/png" />
    <meta property="og:type" content="article">
<meta property="og:title" content="為什麼torch.nn.transformer中每個input的feature size需要是head數量的倍數">
<meta property="og:url" content="https://threemonth03.github.io/2023/07/14/2023-07-14-torch.nn.transformer%E7%9A%84%E7%B4%B0%E7%AF%80/index.html">
<meta property="og:site_name" content="三個月的小屋">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-6bdaf739fd6b827b2087b4e151c560f4_720w.webp">
<meta property="og:image" content="https://pic4.zhimg.com/v2-35d78d9aa9150ae4babd0ea6aa68d113_r.jpg">
<meta property="og:image" content="https://i.imgur.com/H2UvQ1E.png">
<meta property="article:published_time" content="2023-07-14T09:29:00.000Z">
<meta property="article:modified_time" content="2023-07-14T13:38:21.098Z">
<meta property="article:author" content="Three Month">
<meta property="article:tag" content="阿克補習班">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic1.zhimg.com/80/v2-6bdaf739fd6b827b2087b4e151c560f4_720w.webp">
    
<link rel="stylesheet" href="/lib/fancybox/fancybox.css">
<link rel="stylesheet" href="/lib/justifiedGallery/justifiedGallery.min.css">
<link rel="stylesheet" href="/lib/mdui_043tiny/mdui.css">


    <link rel="stylesheet" href="/lib/iconfont/iconfont.css?v=1695719281310">
    
    <link rel="stylesheet" href="/css/style.css?v=1695719281310">

    
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body class="mdui-drawer-body-left">
    <div id="nexmoe-background">
        <div class="nexmoe-bg" style="background-image: url(/images/background/newbg.jpg)"></div>
        <div class="nexmoe-small" style="background-image: url(/images/background/Night.jpg)"></div>
        <div class="mdui-appbar mdui-shadow-0">
            <div class="mdui-toolbar">
                <a mdui-drawer="{target: '#drawer', swipe: true}" title="menu" class="mdui-btn mdui-btn-icon mdui-ripple"><i class="mdui-icon nexmoefont icon-menu"></i></a>
                <div class="mdui-toolbar-spacer"></div>
                <!--<a href="javascript:;" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">search</i></a>-->
                <a href="/" title="Three Month" class="mdui-btn mdui-btn-icon"><img src="/images/head/new.jpg" alt="Three Month"></a>
            </div>
        </div>
    </div>
    <div id="nexmoe-header">
        <div class="nexmoe-drawer mdui-drawer" id="drawer">
    <div class="nexmoe-avatar mdui-ripple">
        <a href="/" title="Three Month">
            <img src="/images/head/new.jpg" alt="Three Month"
                alt="Three Month">
        </a>
    </div>
    <div class="nexmoe-count">
        <div><span>
                文章
            </span>
            14
        </div>
        <div><span>
                標籤
            </span>
            9
        </div>
        <div><span>
                分類
            </span>
            6
        </div>
    </div>
    <div class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}">
        
            <a class="nexmoe-list-item mdui-list-item mdui-ripple false"
                href="/" title="回到首頁">
                <i
                    class="mdui-list-item-icon nexmoefont icon-home"></i>
                <div class="mdui-list-item-content">
                    回到首頁
                </div>
            </a>
            
            <a class="nexmoe-list-item mdui-list-item mdui-ripple false"
                href="/archives.html" title="文章歸檔">
                <i
                    class="mdui-list-item-icon nexmoefont icon-i-catalog"></i>
                <div class="mdui-list-item-content">
                    文章歸檔
                </div>
            </a>
            
            <a class="nexmoe-list-item mdui-list-item mdui-ripple false"
                href="/about.html" title="關於我">
                <i
                    class="mdui-list-item-icon nexmoefont icon-info-circle"></i>
                <div class="mdui-list-item-content">
                    關於我
                </div>
            </a>
            
    </div>
    <aside id="nexmoe-sidebar">
    
        
        <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-search">
         
            <form id="search_form" action_e="https://www.google.com/search?q=site:www.threemonth03.com" onsubmit="return search();">
                <label><input id="search_value" name="q" type="search" placeholder="搜尋"></label>
            </form>
         
    </div>
</div>




    
        
        <div class="nexmoe-widget-wrap">
	<div class="nexmoe-widget nexmoe-social">
		<a
			class="mdui-ripple"
			href="https://twitter.com/Thre3Month"
			target="_blank"
			mdui-tooltip="{content: 'Twitter'}"
			style="
				color: rgb(59, 151, 239);
				background-color: rgba(59, 151, 239, .1);
			"
		>
			<i
				class="nexmoefont icon-twitter"
			></i> </a
		><a
			class="mdui-ripple"
			href="mailto:austin20463@gmail.com"
			target="_blank"
			mdui-tooltip="{content: 'mail'}"
			style="
				color: rgb(249,8,8);
				background-color: rgba(249,8,8,.1);
			"
		>
			<i
				class="nexmoefont icon-mail-fill"
			></i> </a
		><a
			class="mdui-ripple"
			href="https://github.com/threemonth03/"
			target="_blank"
			mdui-tooltip="{content: 'GitHub'}"
			style="
				color: rgb(25, 23, 23);
				background-color: rgba(25, 23, 23, .15);
			"
		>
			<i
				class="nexmoefont icon-github"
			></i> </a
		><a
			class="mdui-ripple"
			href="https://note.threemonth03.com"
			target="_blank"
			mdui-tooltip="{content: 'Note'}"
			style="
				color: rgb(100, 100, 100);
				background-color: rgba(100, 100, 100, .15);
			"
		>
			<i
				class="nexmoefont icon-container"
			></i> </a
		>
	</div>
</div>

    
        
        
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">文章分類</h3>
    <div class="nexmoe-widget">

      <ul class="category-list">

        


        

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/修課心得/">修課心得</a>
          <span class="category-list-count">5</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/個人網站/">個人網站</a>
          <span class="category-list-count">1</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/升學準備/">升學準備</a>
          <span class="category-list-count">1</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/生活/">生活</a>
          <span class="category-list-count">1</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/知識/">知識</a>
          <span class="category-list-count">5</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/面試/">面試</a>
          <span class="category-list-count">1</span>
        </li>

        
      </ul>

    </div>
  </div>


    
        
        
  <div class="nexmoe-widget-wrap">
    <div id="randomtagcloud" class="nexmoe-widget tagcloud nexmoe-rainbow">
      <a href="/tags/%E4%B8%89%E5%80%8B%E6%9C%88%E7%9C%9F%E5%BC%B1/" style="font-size: 10px;">三個月真弱</a> <a href="/tags/%E5%88%9D%E5%AD%B8/" style="font-size: 15px;">初學</a> <a href="/tags/%E5%88%B7%E6%B0%B4%E8%AA%B2/" style="font-size: 15px;">刷水課</a> <a href="/tags/%E5%BB%A2%E6%96%87/" style="font-size: 15px;">廢文</a> <a href="/tags/%E7%88%86%E8%82%9D/" style="font-size: 10px;">爆肝</a> <a href="/tags/%E8%8B%B1%E6%96%87%E8%AA%B2project/" style="font-size: 15px;">英文課project</a> <a href="/tags/%E8%AB%96%E6%96%87%E7%B2%BE%E8%AE%80/" style="font-size: 10px;">論文精讀</a> <a href="/tags/%E8%BD%89%E9%A0%98%E5%9F%9F/" style="font-size: 10px;">轉領域</a> <a href="/tags/%E9%98%BF%E5%85%8B%E8%A3%9C%E7%BF%92%E7%8F%AD/" style="font-size: 20px;">阿克補習班</a>
    </div>
    
      <script>
        var maxTagcloud = parseInt(17);
        var tags_length = parseInt(9);
        var tags_arr = [];
        for(var i = 0; i < tags_length; i++){
          tags_arr.push(i);
        }
        tags_arr.sort(function (l, r) {
          return Math.random() > 0.5 ? -1 : 1;
        });
        tags_arr = tags_arr.slice(0, maxTagcloud < tags_length ? tags_length - maxTagcloud : 0);
        for(var tag_i = 0; tag_i < tags_arr.length; tag_i++){
          document.getElementById("randomtagcloud").children[tags_arr[tag_i]].style.display = 'none';
        }
      </script>
    
  </div>

    
        
        
        
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">所有文章</h3>
    <div class="nexmoe-widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/">2023</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/">2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/">2021</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>



    
        
        
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">最新文章</h3>
    <div class="nexmoe-widget">
      <ul>
        
          <li>
            <a href="/2023/08/27/2023-08-27-%E4%BD%BF%E7%94%A8nni%E6%89%BE%E6%9C%80%E4%BD%B3%E8%B6%85%E5%8F%83%E6%95%B8/">使用nni尋找最佳超參數</a>
          </li>
        
          <li>
            <a href="/2023/07/21/2023-07-21-%E9%80%8F%E9%81%8Edocker%E5%BB%BA%E7%AB%8Bjupyter%E8%88%87tensorboard%E7%92%B0%E5%A2%83/">透過docker建立jupyter與tensorboard環境</a>
          </li>
        
          <li>
            <a href="/2023/07/20/2023-07-20-LLaMA%E4%B8%AD%E4%BD%BF%E7%94%A8%E7%9A%84Positional%20Embedding/">LLaMA中使用的Positional Embedding</a>
          </li>
        
          <li>
            <a href="/2023/07/14/2023-07-14-torch.nn.transformer%E7%9A%84%E7%B4%B0%E7%AF%80/">為什麼torch.nn.transformer中每個input的feature size需要是head數量的倍數</a>
          </li>
        
          <li>
            <a href="/2023/07/01/2023-07-01-%E5%A4%A7%E5%9B%9B%E4%B8%8B%E4%BF%AE%E8%AA%B2%E5%BF%83%E5%BE%97/">大四下修課心得</a>
          </li>
        
      </ul>
    </div>
  </div>

    
        
        <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-link">
		<ul>
        
		</ul>
    </div>
</div>
<style>
.nexmoe-widget-wrap .nexmoe-link ul li a {
    text-align : center;
}
.nexmoe-widget-wrap .nexmoe-link ul li a img {
    max-width : 100%;
}
.nexmoe-widget-wrap .nexmoe-link ul li a p {
    margin: 10px 0;
}
</style>

    
</aside>
        <div class="nexmoe-copyright">
            &copy; 2023
                Three Month
                    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
                        & <a href="https://github.com/theme-nexmoe/hexo-theme-nexmoe" target="_blank">Nexmoe</a>
                        
                            <div style="font-size: 12px">
                                <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
                                本站总访问量 <a id="busuanzi_value_site_pv"></a> 次<br />
                                本站访客数<a id="busuanzi_value_site_uv"></a>人次
                            </div>
                            
                                
        </div>
</div><!-- .nexmoe-drawer -->
    </div>
    <div id="nexmoe-content">
        <div class="nexmoe-primary">
            <div class="nexmoe-post">

  <article>
      
          <div class="nexmoe-post-cover" style="padding-bottom: NaN%;"> 
              <img src="/images/background/transformer.png" alt="為什麼torch.nn.transformer中每個input的feature size需要是head數量的倍數" loading="lazy">
              <h1>為什麼torch.nn.transformer中每個input的feature size需要是head數量的倍數</h1>
          </div>
      
      
      <div class="nexmoe-post-meta nexmoe-rainbow" style="margin:10px 0!important;">
    <a><i class="nexmoefont icon-calendar-fill"></i>2023年07月14日</a>
</div>
      
    <a><i class="nexmoefont icon-areachart"></i>約1.2k字</a>
    <a><i class="nexmoefont icon-time-circle-fill"></i>大概需要5分鐘</a>


      <p><escape><span id="more"></span><escape></escape></escape></p>
<h2 id="前言">前言</h2>
<p>在第一次新生訓練時，學長有講解有關transformer的架構，而講解完後，他留了一個小作業，也就是關於
multi-head
transformer在pytorch中的實作中，d_model需要被nhead整除，d_model是指說input的feature
size，nhead是指說head的數量。這個結果好像跟理論上有些落差，並要我們解釋原因。</p>
<h2 id="multi-head-transformer-理論">Multi-Head Transformer 理論</h2>
<p>在學長的講解過程中，有提到對於Multi-Head
Transformer是對同一個input做了很多個Self-Attention後，再將多次輸出連接在一起後，再和一個大矩陣相乘，壓回原本的大小，這個過程也就是做加權平均。具體過程如下兩張圖。</p>
<p><img data-fancybox="gallery" src="https://pic1.zhimg.com/80/v2-6bdaf739fd6b827b2087b4e151c560f4_720w.webp" alt="Multi-Head Transformer的輸出" data-caption="Multi-Head Transformer的輸出" loading="lazy"></p>
<p><img data-fancybox="gallery" src="https://pic4.zhimg.com/v2-35d78d9aa9150ae4babd0ea6aa68d113_r.jpg" alt="將多個輸出壓回一個輸出的過程" data-caption="將多個輸出壓回一個輸出的過程" loading="lazy"></p>
<p>而上述的過程中，我們會發現無論輸入x的size是多少，都與head的數量無關，也就是說不論feature
size和head數量是多少，理論上都能訓練得起來。</p>
<h2 id="為什麼torchnntransformer中d_model需要被nhead整除">為什麼torch.nn.transformer中d_model需要被nhead整除</h2>
<p>上個段落講說其實feature
size和head數量無關，但如果對nn.transformer填入任意的feature
size和head數量，可能會被提示說feature size需為head數量的倍數。</p>
<p><img data-fancybox="gallery" src="https://i.imgur.com/H2UvQ1E.png" alt="nn.transformer的報錯範例" data-caption="nn.transformer的報錯範例" loading="lazy"></p>
<p>至於為什麼，現在慢慢trace source code。<br>
首先先看到nn.transformer: <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer" class="uri">https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer</a><br>
其中與nhead和d_model相關的部分則是TransformerEncoderLayer這個class。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Transformer</span>(<span class="hljs-title class_ inherited__">Module</span>):<br>    <span class="hljs-comment">#......</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model: <span class="hljs-built_in">int</span> = <span class="hljs-number">512</span>, nhead: <span class="hljs-built_in">int</span> = <span class="hljs-number">8</span>, num_encoder_layers: <span class="hljs-built_in">int</span> = <span class="hljs-number">6</span>,</span><br><span class="hljs-params">                 num_decoder_layers: <span class="hljs-built_in">int</span> = <span class="hljs-number">6</span>, dim_feedforward: <span class="hljs-built_in">int</span> = <span class="hljs-number">2048</span>, dropout: <span class="hljs-built_in">float</span> = <span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">                 activation: <span class="hljs-type">Union</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Callable</span>[[Tensor], Tensor]] = F.relu,</span><br><span class="hljs-params">                 custom_encoder: <span class="hljs-type">Optional</span>[<span class="hljs-type">Any</span>] = <span class="hljs-literal">None</span>, custom_decoder: <span class="hljs-type">Optional</span>[<span class="hljs-type">Any</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                 layer_norm_eps: <span class="hljs-built_in">float</span> = <span class="hljs-number">1e-5</span>, batch_first: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>, norm_first: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">                 device=<span class="hljs-literal">None</span>, dtype=<span class="hljs-literal">None</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        factory_kwargs = {<span class="hljs-string">'device'</span>: device, <span class="hljs-string">'dtype'</span>: dtype}<br>        <span class="hljs-built_in">super</span>().__init__()<br>        torch._C._log_api_usage_once(<span class="hljs-string">f"torch.nn.modules.<span class="hljs-subst">{self.__class__.__name__}</span>"</span>)<br><br>        <span class="hljs-keyword">if</span> custom_encoder <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            self.encoder = custom_encoder<br>        <span class="hljs-keyword">else</span>:<br>            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,<br>                                                    activation, layer_norm_eps, batch_first, norm_first,<br>                                                    **factory_kwargs)<br>            encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)<br>            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)<br>        <span class="hljs-comment">### ......</span><br></code></pre></td></tr></table></figure>
<p>所以繼續看class TransformerEncoderLayer到底寫了什麼，我們發現在class
TransformerEncoderLayer中與d_model和nhead相關的class為MultiheadAttention。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerEncoderLayer</span>(<span class="hljs-title class_ inherited__">Module</span>):<br>    <span class="hljs-comment">#......</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model: <span class="hljs-built_in">int</span>, nhead: <span class="hljs-built_in">int</span>, dim_feedforward: <span class="hljs-built_in">int</span> = <span class="hljs-number">2048</span>, dropout: <span class="hljs-built_in">float</span> = <span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">                 activation: <span class="hljs-type">Union</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Callable</span>[[Tensor], Tensor]] = F.relu,</span><br><span class="hljs-params">                 layer_norm_eps: <span class="hljs-built_in">float</span> = <span class="hljs-number">1e-5</span>, batch_first: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>, norm_first: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">                 device=<span class="hljs-literal">None</span>, dtype=<span class="hljs-literal">None</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        factory_kwargs = {<span class="hljs-string">'device'</span>: device, <span class="hljs-string">'dtype'</span>: dtype}<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,<br>                                            **factory_kwargs)<br>        <span class="hljs-comment">#......</span><br>    <span class="hljs-comment">#......</span><br></code></pre></td></tr></table></figure>
<p>為了繼續trace code，我又在網路上找了class MultiheadAttention: <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention" class="uri">https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention</a><br>
仔細看，他又把forward的過程丟給了F.multi_head_attention_forward。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiheadAttention</span>(<span class="hljs-title class_ inherited__">Module</span>):<br>    <span class="hljs-comment">#.......</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">            self,</span><br><span class="hljs-params">            query: Tensor,</span><br><span class="hljs-params">            key: Tensor,</span><br><span class="hljs-params">            value: Tensor,</span><br><span class="hljs-params">            key_padding_mask: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">            need_weights: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">            attn_mask: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">            average_attn_weights: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">            is_causal : <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span></span>) -&gt; <span class="hljs-type">Tuple</span>[Tensor, <span class="hljs-type">Optional</span>[Tensor]]:<br>        <span class="hljs-comment">#.......</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self._qkv_same_embed_dim:<br>            attn_output, attn_output_weights = F.multi_head_attention_forward(<br>                query, key, value, self.embed_dim, self.num_heads,<br>                self.in_proj_weight, self.in_proj_bias,<br>                self.bias_k, self.bias_v, self.add_zero_attn,<br>                self.dropout, self.out_proj.weight, self.out_proj.bias,<br>                training=self.training,<br>                key_padding_mask=key_padding_mask, need_weights=need_weights,<br>                attn_mask=attn_mask,<br>                use_separate_proj_weight=<span class="hljs-literal">True</span>,<br>                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,<br>                v_proj_weight=self.v_proj_weight,<br>                average_attn_weights=average_attn_weights,<br>                is_causal=is_causal)<br>        <span class="hljs-keyword">else</span>:<br>            attn_output, attn_output_weights = F.multi_head_attention_forward(<br>                query, key, value, self.embed_dim, self.num_heads,<br>                self.in_proj_weight, self.in_proj_bias,<br>                self.bias_k, self.bias_v, self.add_zero_attn,<br>                self.dropout, self.out_proj.weight, self.out_proj.bias,<br>                training=self.training,<br>                key_padding_mask=key_padding_mask,<br>                need_weights=need_weights,<br>                attn_mask=attn_mask,<br>                average_attn_weights=average_attn_weights,<br>                is_causal=is_causal)<br>        <span class="hljs-keyword">if</span> self.batch_first <span class="hljs-keyword">and</span> is_batched:<br>            <span class="hljs-keyword">return</span> attn_output.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>), attn_output_weights<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> attn_output, attn_output_weights<br>    <span class="hljs-comment">#......</span><br></code></pre></td></tr></table></figure>
<p>所以我們再去看F.multi_head_attention_forward寫了什麼: <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/main/torch/nn/functional.py" class="uri">https://github.com/pytorch/pytorch/blob/main/torch/nn/functional.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">multi_head_attention_forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    query: Tensor,</span><br><span class="hljs-params">    key: Tensor,</span><br><span class="hljs-params">    value: Tensor,</span><br><span class="hljs-params">    embed_dim_to_check: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    num_heads: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    <span class="hljs-comment">#......</span></span><br><span class="hljs-params">    </span>)<br>    <span class="hljs-comment">#......</span><br>    <span class="hljs-keyword">else</span>:<br>        head_dim = embed_dim // num_heads<br>    <span class="hljs-keyword">assert</span> head_dim * num_heads == embed_dim, <span class="hljs-string">f"embed_dim <span class="hljs-subst">{embed_dim}</span> not divisible by num_heads <span class="hljs-subst">{num_heads}</span>"</span><br>    <span class="hljs-comment">#......</span><br>    q = q.view(bsz, num_heads, tgt_len, head_dim)<br>        k = k.view(bsz, num_heads, src_len, head_dim)<br>        v = v.view(bsz, num_heads, src_len, head_dim)<br><br>        attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)<br>        attn_output = attn_output.permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>).contiguous().view(bsz * tgt_len, embed_dim)<br><br>        attn_output = linear(attn_output, out_proj_weight, out_proj_bias)<br>        attn_output = attn_output.view(tgt_len, bsz, attn_output.size(<span class="hljs-number">1</span>))<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> is_batched:<br>            <span class="hljs-comment"># squeeze the output if input was unbatched</span><br>            attn_output = attn_output.squeeze(<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> attn_output, <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure>
<p>首先會看到他寫了nhead必須要被embed_dim整除，然後整除後的變數叫做head_dim。</p>
<p>然後順著有head_dim變數的地方找，發現在最後一段中，qkv都變成了4維陣列，再丟進scaled_dot_product_attention運算。<br>
可以仔細觀察四維陣列形狀，分別都是由(batch size, number of head,
target/source length, head
dimension)組成。如果根據上一段的理論來說，格式應該是(batch size,
target/source length, feature size = number of head * head
dimension)。在這不難看出qkv變成了4維陣列的理由，就是單純對target/source
length, head dimension做矩陣乘法運算，而number of
head那個dimension就像是batch size那個dimension一樣，完全不受影響。</p>
<p>簡潔來說torch.nn.transformer的multi-head實作，透過將feature
size平均拆成多份，並將每份分給不同的head做運算，最後再接起來。這種作法的好處就是可以在稍微影響效能的情形下，省去大量的運算，因為每個input平均分給不同的head計算，不會有重複餵input再加權平均的情況發生。假如multi-head是8的話，torch.nn.transformer在這部分減少了約7/8的計算量。</p>
<h2 id="心得">心得</h2>
<p>在這次trace
code的過程中，除了發現理論和實作上的差距，我還發現官方的code，oop寫的比我在交大修的任何AI課還要嚴謹許多，完全不是單純把model刻出來就結束了。在封裝上也很漂亮，也寫了許多assert來規範奇怪的輸入。透過trace
code的過程，也能學習到良好的coding習慣。</p>

      
  </article>

  
      
    <div class="nexmoe-post-copyright">
        <strong>本文作者：</strong>Three Month<br>
        <strong>本文連結：</strong><a href="https://threemonth03.github.io/2023/07/14/2023-07-14-torch.nn.transformer%E7%9A%84%E7%B4%B0%E7%AF%80/" title="https:&#x2F;&#x2F;threemonth03.github.io&#x2F;2023&#x2F;07&#x2F;14&#x2F;2023-07-14-torch.nn.transformer%E7%9A%84%E7%B4%B0%E7%AF%80&#x2F;" target="_blank" rel="noopener">https:&#x2F;&#x2F;threemonth03.github.io&#x2F;2023&#x2F;07&#x2F;14&#x2F;2023-07-14-torch.nn.transformer%E7%9A%84%E7%B4%B0%E7%AF%80&#x2F;</a><br>
        
            <strong>版權聲明：</strong>本文使用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/cn/deed.zh" target="_blank">CC BY-NC-SA 3.0 CN</a> 協議進行許可
        
    </div>


  
  
  <div class="nexmoe-post-meta nexmoe-rainbow">
    
        <a class="nexmoefont icon-appstore-fill -link" href="/categories/%E7%9F%A5%E8%AD%98/">知識</a>
    
    
        <a class="nexmoefont icon-tag-fill -none-link" href="/tags/%E9%98%BF%E5%85%8B%E8%A3%9C%E7%BF%92%E7%8F%AD/" rel="tag">阿克補習班</a>
    
</div>

  
    <script async src="/js/copy-codeblock.js?v=1695719281246"></script>
  

  
      <div class="nexmoe-post-footer">
          <section class="nexmoe-comment">
    <script id="dsq-count-scr" src="https://threemonth03.disqus.com/count.js" async></script>
<div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
        this.page.url = 'https://threemonth03.github.io/2023/07/14/2023-07-14-torch.nn.transformer%E7%9A%84%E7%B4%B0%E7%AF%80/'; // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://threemonth03.github.io/2023/07/14/2023-07-14-torch.nn.transformer%E7%9A%84%E7%B4%B0%E7%AF%80/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>
<script id="disqus-thread-script">
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//threemonth03.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

</section>
      </div>
  
</div>
            <div class="nexmoe-post-right">
              <div class="nexmoe-fixed">
                  <div class="nexmoe-tool"> 
                    
                      
                        
                          
                          
                              <button class="mdui-fab catalog" style="overflow:unset;">
                                  <i class="nexmoefont icon-i-catalog"></i>
                                  <div class="nexmoe-toc">
                                      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#multi-head-transformer-%E7%90%86%E8%AB%96"><span class="toc-number">2.</span> <span class="toc-text">Multi-Head Transformer 理論</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%82%BA%E4%BB%80%E9%BA%BCtorchnntransformer%E4%B8%ADd_model%E9%9C%80%E8%A6%81%E8%A2%ABnhead%E6%95%B4%E9%99%A4"><span class="toc-number">3.</span> <span class="toc-text">為什麼torch.nn.transformer中d_model需要被nhead整除</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BF%83%E5%BE%97"><span class="toc-number">4.</span> <span class="toc-text">心得</span></a></li></ol>
                                  </div>
                              </button>
                          
                          
                      
                    
                      <a href="#nexmoe-content" class="toc-link" aria-label="Back To Top" title="top"><button class="mdui-fab mdui-ripple"><i class="nexmoefont icon-caret-top"></i></button></a>
                  </div>
              </div>
            </div>
        </div>
    </div>
    <div id="nexmoe-search-space">
	<div class="search-container">
		<div class="search-header">
			<div class="search-input-container">
				<input
					class="search-input"
					type="text"
					placeholder="搜尋"
					oninput="sinput();"
				/>
			</div>
			<a class="search-close" onclick="sclose();">×</a>
		</div>
		<div class="search-body"></div>
	</div>
</div>

    
<script src="/lib/mdui_043tiny/mdui.js"></script>
<script src="/lib/jquery.min.js"></script>
<script src="/lib/justifiedGallery/jquery.justifiedGallery.min.js"></script>
<script src="/lib/fancybox/fancybox.umd.js"></script>


 

<script async src="/js/app.js?v=1695719281312"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2058306854838448" crossorigin="anonymous"></script>

<script>
	$(".justified-gallery").justifiedGallery({
		rowHeight: 160,
		margins: 10,
	});
</script>

</body>

</html>
